name <- c('outlier_border', '95th_quantile', 'mean')
value <- c(boxplot.stats(metric)$stats[1],as.numeric(quantile(metric,.95)),(boxplot.stats(metric)$stats[5] + as.numeric(quantile(metric,.95))) /2)
data.frame(name,value)
metric = (step2$per_project_admins + step2$per_jira_admins)
ggplot(step2, aes(x=metric,color=as.factor(jira_reporting_status))) +
geom_density()+
scale_color_manual(values=c(atlassian_palettes$multi_long,atlassian_palettes$multi_dark)) +
#facet_grid(month~.,scales = "free")+
geom_vline(aes(xintercept=8.0),color=atlassian_palettes$rainbow[3], linetype="dashed", size=1)+
#scale_x_continuous(limits = c(0,as.numeric(quantile(step2$unique_users,0.95))),breaks = seq(0,as.numeric(quantile(step2$unique_users,0.95)),by=10))+
atlassian_theme +
labs(title = 'Proportion of admins (Density)',
y = 'density',
x = '% admins')
name <- c('outlier_border', '95th_quantile', 'mean')
value <- c(boxplot.stats(metric)$stats[1],as.numeric(quantile(metric,.95)),(boxplot.stats(metric)$stats[5] + as.numeric(quantile(metric,.95))) /2)
data.frame(name,value)
metric = (step2$per_project_admins + step2$per_jira_admins)
ggplot(step2, aes(x=metric,color=as.factor(jira_reporting_status))) +
geom_density()+
scale_color_manual(values=c(atlassian_palettes$multi_long,atlassian_palettes$multi_dark)) +
#facet_grid(month~.,scales = "free")+
geom_vline(aes(xintercept=15.0),color=atlassian_palettes$rainbow[3], linetype="dashed", size=1)+
#scale_x_continuous(limits = c(0,as.numeric(quantile(step2$unique_users,0.95))),breaks = seq(0,as.numeric(quantile(step2$unique_users,0.95)),by=10))+
atlassian_theme +
labs(title = 'Proportion of admins (Density)',
y = 'density',
x = '% admins')
step3 <- df %>% subset(jira_reporting_status == 'production' & (per_project_admins + per_jira_admins) < 15)
name <- c('before','after','diff')
value <- c(round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(step2$parent_sen,step2$instance))),1))
data.frame(name,value)
step3 <- df %>% subset(jira_reporting_status == 'production' & (per_project_admins + per_jira_admins) < 15)
name <- c('before','after','step_over_step','diff_total')
value <- c(round(length(unique(paste0(step3$parent_sen,step3$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(step2$parent_sen,step2$instance))),1),
round(length(unique(paste0(step2$parent_sen,step2$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
value <- c(round(length(unique(paste0(step3$parent_sen,step3$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(step2$parent_sen,step2$instance))),1),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
name <- c('before','after','step_over_step','diff_total')
value <- c(round(length(unique(paste0(df$parent_sen,df$instance))),0),
round(length(unique(paste0(step1$parent_sen,step1$instance))),0),
round(length(unique(paste0(step1$parent_sen,step1$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1),
round(length(unique(paste0(step1$parent_sen,step1$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
name <- c('before','after','step_over_step','diff_total')
value <- c(round(length(unique(paste0(step1$parent_sen,step1$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))) * 100.0 / length(unique(paste0(step1$parent_sen,step1$instance))),1),
round(length(unique(paste0(step2$parent_sen,step2$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
name <- c('before','after','step_over_step','diff_total')
value <- c(round(length(unique(paste0(step3$parent_sen,step3$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(step2$parent_sen,step2$instance))),1),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
length(unique(step3$instance)) / length(unique(step3$parent_sen))
length(unique(paste0(df$parent_sen,df$instance))) / length(unique(df$parent_sen))
length(unique(paste0(step3$parent_sen,step3$instance))) / length(unique(step3$parent_sen))
name <- c('before','after','step_over_step','diff_total')
value <- c(round(length(unique(paste0(step3$parent_sen,step3$instance))),0),
round(length(unique(paste0(step2$parent_sen,step2$instance))),0),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(step2$parent_sen,step2$instance))),1),
round(length(unique(paste0(step3$parent_sen,step3$instance))) * 100.0 / length(unique(paste0(df$parent_sen,df$instance))),1))
data.frame(name,value)
length(unique(paste0(df$parent_sen,df$instance))) / length(unique(df$parent_sen))
length(unique(paste0(step3$parent_sen,step3$instance))) / length(unique(step3$parent_sen))
metric = (step2$per_project_admins + step2$per_jira_admins)
ggplot(step2, aes(x=metric,color=as.factor(jira_reporting_status))) +
geom_density()+
scale_color_manual(values=c(atlassian_palettes$multi_long,atlassian_palettes$multi_dark)) +
#facet_grid(month~.,scales = "free")+
geom_vline(aes(xintercept=15.0),color=atlassian_palettes$rainbow[3], linetype="dashed", size=1)+
#scale_x_continuous(limits = c(0,as.numeric(quantile(step2$unique_users,0.95))),breaks = seq(0,as.numeric(quantile(step2$unique_users,0.95)),by=10))+
atlassian_theme +
labs(title = 'Proportion of admins (Density)',
y = 'density',
x = '% admins')
str(df)
install.packages(c('odbc', 'tidyverse', 'awsjavasdk'))
install.packages(c("odbc", "tidyverse", "awsjavasdk"))
install.packages(c("odbc", "tidyverse", "awsjavasdk"))
library(odbc)
library(tidyverse)
library(rJava)
library(awsjavasdk)
awsjavasdk::load_sdk()
library(odbc)
install.packages("odbc")
library(odbc)
library(tidyverse)
library(rJava)
library(awsjavasdk)
creds <- .jnew("com.amazonaws.auth.DefaultAWSCredentialsProviderChain")
Sys.setenv(
AWS_ACCESS_KEY_ID = creds$getCredentials()$getAWSAccessKeyId(),
AWS_SECRET_ACCESS_KEY = creds$getCredentials()$getAWSSecretKey(),
AWS_SESSION_TOKEN = creds$getCredentials()$getSessionToken())
zone_set <- DBI::dbConnect(
odbc::odbc(),
driver = "/Library/simba/athenaodbc/lib/libathenaodbc_sbu.dylib",
schema = "zone_set",
AwsRegion = "us-east-1",
AuthenticationType = "Default Credentials",
S3OutputLocation = "s3://atl-ai-athena-query-results/"
)
R.Version()
tmp <- installed.packages()
installedpkgs <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
save(installedpkgs, file="installed_old.rda")
getwd()
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = '@Lebronjames23')
install.packages(as.vector(needed_packages))
tmp <- installed.packages()
installedpkgs.new <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
missing <- setdiff(installedpkgs, installedpkgs.new)
install.packages(missing)
update.packages()
install.packages(c('odbc', 'tidyverse', 'awsjavasdk'))
library(odbc)
install.packages("odbc")
library(odbc)
library(odbc)
library(Rcpp)
install.packages("Rcpp")
library(odbc)
install.packages("bit64")
library(odbc)
install.packages("bit")
library(odbc)
install.packages("tibble")
library(odbc)
install.packages("rlang")
library(odbc)
library(tidyverse)
install.packages("dplyr")
library(tidyverse)
install.packages("bindrcpp")
library(tidyverse)
install.packages("glue")
library(tidyverse)
install.packages("tidyselect")
library(tidyverse)
install.packages("purrr")
library(tidyverse)
install.packages("plyr")
library(tidyverse)
install.packages("mnormt")
library(tidyverse)
install.packages("reshape2")
library(tidyverse)
install.packages("stringi")
library(tidyverse)
install.packages("tidyr")
library(tidyverse)
install.packages("scales")
library(tidyverse)
install.packages("colorspace")
library(tidyverse)
install.packages("lazyeval")
library(tidyverse)
install.packages("haven")
library(tidyverse)
install.packages("jsonlite")
library(tidyverse)
tidyverse_update(recursive = FALSE)
install.packages("lubridate")
library(tidyverse)
install.packages("readr")
library(tidyverse)
install.packages("readx1")
library(tidyverse)
install.packages("readx1")
install.packages("readx1")
library(tidyverse)
install.packages('readx1', dependencies=TRUE, repos='http://cran.rstudio.com/')
library(tidyverse)
install.packages(readx1)
install.packages(c("Amelia", "backports", "blob", "broom", "car", "carData", "caret", "caTools", "checkmate", "chron", "cli", "covr", "cowplot", "curl", "CVST", "data.table", "ddalpha", "dendextend", "dimRed", "doBy", "dotCall64", "e1071", "effects", "evaluate", "FactoMineR", "fields", "forcats", "forecast", "Formula", "fpc", "gdtools", "GGally", "ggfortify", "ggpubr", "ggrepel", "ggsci", "ggthemes", "gmodels", "googleVis", "gsubfn", "gtools", "hexbin", "highr", "HistData", "hms", "htmltab", "htmlTable", "htmlwidgets", "httpuv", "infuser", "ipred", "iterators", "kernlab", "knitr", "koRpus", "lava", "leaflet", "lexicon", "lme4", "lmtest", "mapproj", "maps", "maptools", "mclust", "mime", "miniUI", "ModelMetrics", "modelr", "modeltools", "multcomp", "mvtnorm", "nloptr", "NLP", "openssl", "packrat", "pacman", "PerformanceAnalytics", "pillar", "plotly", "prodlim", "progress", "pscl", "psych", "quantmod", "quantreg", "R.oo", "R.utils", "R6", "randomForest", "raster", "RcppArmadillo", "RcppEigen", "RcppRoll", "RCurl", "readxl", "recipes", "reshape", "rex", "RJSONIO", "rmarkdown", "robustbase", "rpart.plot", "RPresto", "rprojroot", "rsconnect", "RSQLite", "rstudioapi", "sandwich", "scatterplot3d", "selectr", "sentimentr", "sfsmisc", "shiny", "shinydashboard", "sourcetools", "sp", "spam", "stringr", "survey", "testthat", "TH.data", "timeDate", "TraMineR", "trimcluster", "tseries", "TSstudio", "TTR", "UsingR", "vegan", "viridis", "viridisLite", "XML", "xtable", "xts", "yaml", "zoo"))
R.Version()
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(odbc)
install.packages(odbc)
install.packages("odbc")
library(odbc)
install.packages("DBI")
library(odbc)
install.packages("DBI")
library(odbc)
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
library(tidyverse)
library(rJava)
install.packages("rJava)
install.packages("rJava")
library(rJava)
install.packages("rJava")
library(rJava)
install.packages("rJava")
install.packages("rJava")
library(rJava)
library(rJava)
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
#setwd('/Users/mickey/Documents/GitHub/data-science-capstone')
setwd('/Users/mebner/Documents/for_me/R_coursera/data-science-capstone')
path_to_data <- '/Users/mebner/Documents/for_me/R_coursera/final/'
#table length function
#et_counts <- function (filename) {
# path <- paste0('final/',substring(filename, 1,5),'/',filename)
#con <- file(path,"r")
#set.seed(23)
#full <- readLines(con, encoding="UTF-8")
#print(paste0(filename,'|number of lines: ',length(full)))
#print(paste0(filename,'|number of words: ',(sum(lengths(strsplit(full, "\\W+"))))))
#close(con)
#}
#run getsample for each file selectd
#get_counts('en_US.blogs.txt')
#get_counts('en_US.news.txt')
#suppressWarnings(get_counts('en_US.twitter.txt'))
#for function testing only
filename = 'en_US.news.txt'
prob = 0.05
get_sample <- function (filename,prob) {
path <- paste0(path_to_data,substring(filename, 1,5),'/',filename)
# load Google's list of bad words
con <- file(path,"r")
full <- readLines(con, encoding="UTF-8")
close(con)
set.seed(23)
sample <- as.data.frame(list(full[rbinom(length(full),1,prob)==1]))
colnames(sample)[1] <- "words"
sample
}
clean_data <- function(sample) {
sample <- sample %>%mutate(words = gsub("(http[s]?://|www)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+ ", "", words)) %>%
mutate(words = gsub("@\\w+", " ", words)) %>%
mutate(words = removePunctuation(words,preserve_intra_word_contractions = TRUE,preserve_intra_word_dashes = TRUE)) %>%
mutate(words = gsub("[^\x20-\x7E]", "", words)) %>%
mutate(words = gsub("  "," ",words)) %>%
mutate(words = gsub("n\'t", " not", words)) %>%
mutate(words = gsub("\'re", " are", words)) %>%
mutate(words = gsub("\'s", " is", words)) %>%
mutate(words = gsub("\'d", " would", words)) %>%
mutate(words = gsub("\'ll", " will", words)) %>%
mutate(words = gsub("\'t", " not", words)) %>%
mutate(words = gsub("\'ve", " have", words)) %>%
mutate(words = gsub("\'m", " am", words))
#tokinize the text, get words, 2-grams and 3-grams. We keep stopwords as they are relevant for this task
sample <- as.vector(sample[,1]) %>%
tokenize_ngrams(n = 4, n_min = 1) %>%
unlist() %>% list() %>%
as.data.frame() %>% mutate(language = str_sub(filename,1,5),media = str_extract(filename,'blogs|news|twitter'))
colnames(sample)[1] <- "words"
#sample file with gram type and bad word column
sample <- sample %>% mutate(type = paste0(str_count(words, ' ')+1,'-grams'),bad_word = grepl(paste0('((^| )',unlist(en_bad),'( |$))', collapse="|"),words))
#get rid of numbers only recoreds and records that end with a number
sample <- sample %>% subset(!grepl('^[0-9, ]+$',words) & !grepl('.*[0-9, ]$',words)) %>%
mutate(words = gsub("\\d+", "<digit>", words))
#get frequencies without bad words and write file
}
getfrequencies <- function(cleaned_sample) {
en_bad <- read.csv("badlanguage.txt", header = T)
freq <- cleaned_sample %>% subset(bad_word = 'FALSE') %>% group_by(words,language,media,type) %>%
summarise(counts = n()) %>% ungroup() %>% group_by(language,media,type) %>%
mutate(rank = rank(-counts)) %>%
ungroup()
}
#possible filenames
files <- list.files("final",recursive=TRUE, full.names=TRUE, pattern="*/*\\.txt")
gsub('[a-z]{5}\\/[a-z]{2}_[A-Z]{2}\\/','',files)
#possible sample values are > 0 and < 1
prob = 0.05
#run getsample for each file selectd
blog_sample <- get_sample('en_US.blogs.txt',prob)
news_sample <- get_sample('en_US.news.txt',prob)
twitter_sample <- get_sample('en_US.twitter.txt',prob)
#run getsample for each file selectd
blog_sample <- clean_data(blog_sample)
news_sample <- clean_data(news_sample)
twitter_sample <- clean_data(twitter_sample)
sample <- rbind(blog_sample %>% subset(bad_word = 'FALSE'),news_sample %>% subset(bad_word = 'FALSE'),twitter_sample %>% subset(bad_word = 'FALSE'))
#read freq files
blog_freq <- getfrequencies(blog_sample)
news_freq <- getfrequencies(news_sample)
twitter_freq <- getfrequencies(twitter_sample)
freq <- rbind(blog_freq,news_freq,twitter_freq)
sample_freq <- sample %>%
select(words,language,type) %>%
group_by(words,language,type) %>%
summarise(counts = n()) %>%
group_by(language,type) %>%
mutate(rank = rank(-counts, ties.method = 'first')) %>%
as.data.frame()
#list data frames
df <- sample_freq %>%
#subset(grepl('^zurich',words)) %>%
select(words,type,counts,language) %>%
arrange(-counts) %>%
group_by(type,language) %>%
mutate(total = sum(counts), freq = counts / sum(counts), unique_count = 1) %>%
mutate(cum_unique = cumsum(unique_count),cum_freq = cumsum(freq))
df_sample_95 <- df %>%
subset(cum_freq <= .95 & type != 'NA-grams') %>%
mutate(input = str_replace(words,paste0(' ',str_extract(words,"[<,>,a-z]+$")),""),
next_word = str_extract(words,"[<,>,a-z]+$")) %>%
subset(next_word != '<digit>') %>%
select(type,input,next_word,counts) %>%
group_by(type, input) %>%
mutate(n = sum(counts)) %>%
distinct(type, input, next_word, n, .keep_all=TRUE) %>%
mutate(prob = round(counts / n,2))
#ldf <- split(df,df$type,drop=FALSE)
#ldf <- lapply(df, function(x) {
#  mutate(x,
#         input =str_replace(words,paste0(' ',str_extract(words,"[<,>,a-z]+$")),""),
#         last_word = str_extract(words,"[<,>,a-z]+$"))
#})
df <- df_sample
predict_next_word <- function(user_input) {
#remove urls
user_input <- gsub("(http[s]?://|www)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", user_input)
#remove twitter add mentions
user_input <- gsub("@\\w+", "", user_input)
#remove punctuation
user_input <- removePunctuation(user_input,preserve_intra_word_contractions = TRUE,preserve_intra_word_dashes = TRUE)
#remove records with non-asci characters
user_input <- gsub("[^\x20-\x7E]", "", user_input)
#to lower
user_input <- tolower(user_input)
#remove double spaces
user_input <- gsub("  "," ",user_input)
#replace digits with placeholder
user_input = gsub("\\d+", "<digit>", user_input)
if(!is.na(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[1,4])){
print(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[1,4])
print(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[2,4])
print(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[3,4])
print(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[1,3])
print(as.data.frame(head(df %>% subset(type == '4-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,2}$')) %>% arrange(-prob)))[1,2])
} else if (!is.na(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[1,4])){
print(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[1,4])
print(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[2,4])
print(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[3,4])
print(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[1,3])
print(as.data.frame(head(df %>% subset(type == '3-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,1}$')) %>% arrange(-prob)))[1,2])
} else if (!is.na(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[1,4])){
print(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[1,4])
print(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[2,4])
print(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[3,4])
print(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[1,3])
print(as.data.frame(head(df %>% subset(type == '2-grams' & input == str_extract(user_input,'\\S+(?:\\s+\\S+){0,0}$')) %>% arrange(-prob)))[1,2])
} else {
print(as.data.frame(head(df %>% subset(type == '1-grams') %>% arrange(-prob)))[1,4])
print(as.data.frame(head(df %>% subset(type == '1-grams') %>% arrange(-prob)))[2,4])
print(as.data.frame(head(df %>% subset(type == '1-grams') %>% arrange(-prob)))[3,4])
print(as.data.frame(head(df %>% subset(type == '1-grams') %>% arrange(-prob)))[1,3])
print(as.data.frame(head(df %>% subset(type == '1-grams') %>% arrange(-prob)))[1,2])
}
}
user_input <- 'no I just'
start_time <- Sys.time()
user_input <- predict_next_word(user_input)
end_time <- Sys.time()
start_time - end_time
system.time(predict_next_word(user_input))
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
installed.packages("tookenizers")
install.packages("tookenizers")
install.packages("tokenizers")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
install.packages("openNLP")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
install.packages("tm")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
install.packages("scals")
install.packages("textcat")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
install.packages("scals")
install.packages("tm")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
install.packages("textcat")
library(stringr)
library("tokenizers")
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rJava)
library(openNLP)
library(scals)
library(tm)
library(textcat)
user_input <- 'no I just'
start_time <- Sys.time()
user_input <- predict_next_word(user_input)
end_time <- Sys.time()
start_time - end_time
system.time(predict_next_word(user_input))
install.packages("tm")
library(tm)
installed.packages("slam")
install.packages("slam")
install.packages('slam',repos='http://cran.us.r-project.org')
gsub('[[:punct:] ]+',' ','servus#$%#$%du')
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", ""),
"servus!du")
install.packages("gsubfn")
library(gsubfn)
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", ""),
"servus!du")
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", " "),
"servus!du")
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", " "),
"servus!#$$%du")
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", " "),
"servus'du")
gsubfn(pattern = "[[:punct:]]", engine = "R",
replacement = function(x) ifelse(x == "'", "'", " "),
"servus' du")
library(tm)
install.packages(tm)
install.packages("tm")
install.packages("tm",dependencies = T)
library(tm)
install.packages("slam",dependencies = T)
tmp <- installed.packages()
installedpkgs <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
save(installedpkgs, file="installed_old.rda")
