---
title: 'Capstone: Exploratory Analysis'
author: "Michael Ebner"
date: "17/10/2018"
output: html_document
---

## Data Science Capstone: Exploratory Analysis

### Introduction

Plans for Creating a Prediction Algorithm
The author plans to use an analysis of words and n-grams to generate a prediction algorithm. Prediction will be based on calculations of maximum likelihood estimates for words generated by a user. A n-gram model will be used which looks at previous words, up to two words, to generate words which are most likely to appear next. Discounting will be used when unknown words or combination of words are entered by the user. Most likely, I will use Katz backoff to discount probabilities. The prediction algorithm will run on a Shiny App where users will enter in text to receive predictions.

The purpose of project is to build a model that produces the word(s) based on a user input. The prediction will be based on the calculation of maximum likeklihood estimates of words and n-grams generarated by a user. The project will work the combinations of up to three words (3-grams).

The challenges to overcome are:
* Finding the balance between predictive power and performance
* Dealing with unknown words/cominations of words
* Selecting the best statistical model

The final result will be availalbe as web interface (buld with RShiny).

## First look at the data: number of rows

In order to create the sampled data I've created the following 

```{r number of lines for the tables, echo = FALSE}
setwd('/Users/mickey/Documents/GitHub/data-science-capstone')

get_counts <- function (filename) {
  path <- paste0('final/',substring(filename, 1,5),'/',filename)
  con <- file(path,"r")
  set.seed(23)
  full <- readLines(con, encoding="UTF-8")
  print(paste0(filename,'|number of lines: ',length(full)))
  print(paste0(filename,'|number of words: ',(sum(lengths(strsplit(full, "\\W+"))))))
  close(con)
}

#run getsample for each file selectd
get_counts('en_US.blogs.txt')
get_counts('en_US.news.txt')
suppressWarnings(get_counts('en_US.twitter.txt'))
```

One can easily see that the we talking about big data here. Dealing with that much data isn't efficient and also it's not necessary. Next we going to pick a sample of each media. We'll also manipulate the data in a way that serves our purpose.

## Getting and cleaning the data

Since the files are quite large we can move on selecting one sample of each file and cleaning the data by going throug the following steps:

1. Get the full data set
2. Remove URLs and twitter @ mentions and punctuation (in that very order)
3. Pick a subset of the data (I'll pick 3%).
4. Tokenize the data (single words, 2-grams and 3-grams)
5. Remove records which bad language from the list
6. Save the sample data
7. Create a frequency table
8. Save the frequency table

That's what the function below does:

```{r pressure, echo=TRUE}
get_sample <- function (filename,prob) {
  path <- paste0('final/',substring(filename, 1,5),'/',filename)
  # load Google's list of bad words
  en_bad <- read.table("badlanguage.txt")
  # open conection
  con <- file(path,"r")
  set.seed(23)
  full <- readLines(con, encoding="UTF-8")
  # urls
  full <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", full)
  # remove twitter @ mentions
  full <- gsub("@\\w+", " ", full)
  # remove punctuation
  full <- removePunctuation(full,preserve_intra_word_contractions = TRUE,preserve_intra_word_dashes = TRUE)
  # Use a binomial distrubution to get a sample where the sample size equals the value of the variable prob
  sample <- as.data.frame(list(full[rbinom(length(full),1,prob)==1]))
  colnames(sample)[1] <- "words"
  #tokinize the text, get words, 2-grams and 3-grams, I also wanna store stopwords as they are relevant for this task
  sample <- as.vector(sample[,1]) %>%
    tokenize_ngrams(n = 3, n_min = 1) %>%
    unlist() %>% list() %>%
    as.data.frame() %>% mutate(language = str_sub(filename,1,5),media = str_extract(filename,'blogs|news|twitter'))
  colnames(sample)[1] <- "words"
  #delete all lines with bad words
  sample <- sample %>% subset(!grepl(paste(unlist(en_bad), collapse="|"),words)) %>%
                       mutate(type = ifelse(str_count(words, ' ') == 0,'words',paste0(str_count(words, ' ')+1,'-grams')))
  #write sample file
  write.table(sample,paste0(gsub('.txt','',filename),'_sample',str_sub(filename,-4,-1)))
  #get frequencies and write file
  freq <- sample %>% group_by(words,language,media,type) %>% summarise(counts = n()) %>% ungroup() %>% group_by(language,media,type) %>% mutate(rank = rank(-counts)) %>% ungroup()
  write.table(freq,paste0(gsub('.txt','',filename),'_freq',str_sub(filename,-4,-1)))
  close(con)
}
```


## Exploratory Analysis

I've pulled the data using the function above. Let's have a look at total counts per type:

```{r total counts, echo=FALSE}

library(stringr)
library(dplyr)
library(ggplot2)
library(gridExtra)


#read freq files
en_blog_freq <- read.table("en_US.blogs_freq.txt",stringsAsFactors=FALSE)
en_news_freq <- read.table("en_US.news_freq.txt",stringsAsFactors=FALSE)
en_twitter_freq <- read.table("en_US.twitter_freq.txt",stringsAsFactors=FALSE)

en_freq <- rbind(en_blog_freq,en_news_freq,en_twitter_freq)

#read samples
en_blog_sample <- read.table("en_US.blogs_sample.txt",stringsAsFactors=FALSE)
en_news_sample <- read.table("en_US.news_sample.txt",stringsAsFactors=FALSE)
en_twitter_sample <- read.table("en_US.twitter_sample.txt",stringsAsFactors=FALSE)

en_sample <- rbind(en_blog_sample,en_news_sample,en_twitter_sample)

#total counts
counts <- en_freq %>% subset(!is.na(type)) %>% group_by(language,media,type) %>% summarise(total_counts = sum(counts)) %>% arrange(media,-total_counts)
total_counts <- en_freq %>% subset(!is.na(type)) %>% group_by(language,type) %>% summarise(total_counts = sum(counts)) %>% mutate(media = 'all') %>% select(language, media, type,total_counts) %>% arrange(media,-total_counts) 
rbind(counts,total_counts)
```

### Top 20 words by usage
```{r, 20 words, echo=FALSE}
#create full data frame
en_sample_freq <- en_sample %>% 
                  select(words,language,type) %>% 
                  group_by(words,language,type) %>% 
                  summarise(counts = n()) %>%
                  group_by(language,type) %>%
                  mutate(rank = rank(-counts, ties.method = 'first')) %>% 
                  as.data.frame()

nw = 20
grams = 'words'

tab <- en_blog_freq %>% subset(type == grams & rank <= nw )
p1 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'blog, en_US',
       x = '# of words',
       y = '')
  
tab <- en_news_freq %>% subset(type == grams & rank <= nw )
p2 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'news, en_US',
       x = '# of words',
       y = '')

tab <- en_twitter_freq %>% subset(type == grams & rank <= nw )
p3 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'twitter, en_US',
       x = '# of words',
       y = '')

tab <- en_sample_freq %>% subset(type == grams & rank <= nw )
p4 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words), colour = "orange") +
  geom_point(colour = "orange") +
  theme_minimal()+
  labs(title = 'total sample, en_US',
       x = '# of words',
       y = '')

grid.arrange(p1, p2, p3, p4, nrow = 2,top = paste0('Top 20 ',grams,' based on frequency'))
```

The whole list contians solely so callded stopwords. Words which are not specific at all and don't give any information about the topic of the conversation. Which makes totall sense.

### Top 20 2-grams by usage
```{r, 20 2-grams, echo=FALSE}
en_sample_freq <- en_sample %>% 
                  select(words,language,type) %>% 
                  group_by(words,language,type) %>% 
                  summarise(counts = n()) %>%
                  group_by(language,type) %>%
                  mutate(rank = rank(-counts, ties.method = 'first')) %>% 
                  as.data.frame()

nw = 20
grams = '2-grams'

tab <- en_blog_freq %>% subset(type == grams & rank <= nw )
p1 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'blog, en_US',
       x = '# of words',
       y = '')
  
tab <- en_news_freq %>% subset(type == grams & rank <= nw )
p2 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'news, en_US',
       x = '# of words',
       y = '')

tab <- en_twitter_freq %>% subset(type == grams & rank <= nw )
p3 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'twitter, en_US',
       x = '# of words',
       y = '')

tab <- en_sample_freq %>% subset(type == grams & rank <= nw )
p4 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words), colour = "orange") +
  geom_point(colour = "orange") +
  theme_minimal()+
  labs(title = 'total sample, en_US',
       x = '# of words',
       y = '')

grid.arrange(p1, p2, p3, p4, nrow = 2,top = paste0('Top 20 ',grams,' based on frequency'))
```

When it comes to the top 20 used 2-grams we also see quite common combinations like 'going to', 'I am' or 'I don't' ranking up there. Which is what I've expected.

### Top 20 3-grams by usage
```{r, 20 3-grams, echo=FALSE}
en_sample_freq <- en_sample %>% 
                  select(words,language,type) %>% 
                  group_by(words,language,type) %>% 
                  summarise(counts = n()) %>%
                  group_by(language,type) %>%
                  mutate(rank = rank(-counts, ties.method = 'first')) %>% 
                  as.data.frame()

nw = 20
grams = '3-grams'

tab <- en_blog_freq %>% subset(type == grams & rank <= nw )
p1 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'blog, en_US',
       x = '# of words',
       y = '')
  
tab <- en_news_freq %>% subset(type == grams & rank <= nw )
p2 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'news, en_US',
       x = '# of words',
       y = '')

tab <- en_twitter_freq %>% subset(type == grams & rank <= nw )
p3 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words)) +
  geom_point() +
  theme_minimal()+
  labs(title = 'twitter, en_US',
       x = '# of words',
       y = '')

tab <- en_sample_freq %>% subset(type == grams & rank <= nw )
p4 <- 
  ggplot(tab, aes(counts, reorder(words,counts,mean))) +
  geom_segment(aes(x = 0, y = reorder(words,counts,mean), xend = counts, yend = words), colour = "orange") +
  geom_point(colour = "orange") +
  theme_minimal()+
  labs(title = 'total sample, en_US',
       x = '# of words',
       y = '')

grid.arrange(p1, p2, p3, p4, nrow = 2,top = paste0('Top 20 ',grams,' based on frequency'))
```

Finally, looking in at the top 20 3-grams it's save to say that the data shaping has been quite a success. Very commenly used phrases rank on top of the list which matches the expectations perfectly.

### How many unique words are needed to cover 50% and 90% of the total words?

```{r, words needed, echo=FALSE}
freq <- en_sample_freq %>% 
        subset(type == 'words') %>%
        select(words,counts,language) %>%
        arrange(-counts) %>%
        group_by(language) %>%
        mutate(total = sum(counts), freq = counts / sum(counts), unique_count = 1) %>%
        mutate(cum_unique = cumsum(unique_count),cum_freq = cumsum(freq))
        

fifty <- nrow(freq %>% subset(cum_freq <= .5))
ninety <- nrow(freq %>% subset(cum_freq <= .9))


ggplot(data=freq, aes(y=cum_freq * 100, x=cum_unique, group=1)) +
  geom_line()+
  geom_hline(yintercept = 50, color = 'orange') +
  geom_hline(yintercept = 90, color = 'blue') +
  geom_vline(xintercept = 154, color = 'orange') +
  geom_vline(xintercept = 8029, color = 'blue') +
  theme_minimal()+
  scale_y_continuous(breaks=seq(0,100,5))+
  scale_x_continuous(breaks=c(fifty,ninety,seq(20000,3096769,10000)))+
  labs(title = 'Number of Unique Words needed to cover 50% and 90% of all words in the corpus',
       x = '# of unique words',
       y = '% of total words')
```

We'd need 154 unique words to cover 50% and around 8k unique words to cover 90% of all words used. That's pretty helpful information and will be needed to balance out performance and precisness.

### What's up next

Next I'm going to pick a statistical model based on the clean data we've got in place now.
Before I go there I might take an even closer look at the data and see if further data cleaning steps are necessary.
I did some research on how to deal with foreign words. While it is quite tricky to identify foreign words, there are some commenly used methods to identify whole sentences written in a foreign language. So I'm considring edding the language detection to my script to create the samples.